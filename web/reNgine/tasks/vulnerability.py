import json
import os
import concurrent.futures
import shlex

from pathlib import Path

from reNgine.definitions import (
    ALL,
    BLIND_XSS_SERVER,
    CRLFUZZ,
    DALFOX,
    DELAY,
    NUCLEI,
    NUCLEI_CUSTOM_TEMPLATE,
    NUCLEI_DEFAULT_SEVERITIES,
    NUCLEI_DEFAULT_TEMPLATES_PATH,
    NUCLEI_SEVERITY,
    NUCLEI_TAGS,
    NUCLEI_TEMPLATE,
    PROVIDERS,
    RETRIES,
    RUN_CRLFUZZ,
    RUN_DALFOX,
    RUN_NUCLEI,
    RUN_S3SCANNER,
    S3SCANNER,
    S3SCANNER_DEFAULT_PROVIDERS,
    THREADS,
    TIMEOUT,
    USE_NUCLEI_CONFIG,
    USER_AGENT,
    VULNERABILITY_SCAN,
    WAF_EVASION,
)
from reNgine.settings import (
    DEFAULT_RETRIES,
    DEFAULT_THREADS,
)
from reNgine.celery import app
from reNgine.celery_custom_task import RengineTask
from reNgine.tasks.notification import send_hackerone_report
from reNgine.tasks.command import run_command_line
from reNgine.utils.logger import Logger
from reNgine.utils.command_executor import stream_command
from reNgine.utils.command_builder import CommandBuilder
from reNgine.utils.task_config import TaskConfig
from reNgine.utils.db import get_vulnerability_gpt_report, save_endpoint, save_vulnerability
from reNgine.utils.notifications import send_vulnerability_notification
from reNgine.utils.http import (
    get_subdomain_from_url,
    parse_curl_output,
    sanitize_url,
    prepare_urls_with_fallback,
)
from reNgine.utils.parsers import (
    parse_s3scanner_result,
    parse_nuclei_result,
    parse_dalfox_result,
    parse_crlfuzz_result,
)
from scanEngine.models import Hackerone
from startScan.models import (
    S3Bucket,
    ScanHistory,
    Subdomain,
    Vulnerability,
)
from dashboard.models import OpenAiAPIKey

logger = Logger(True)

@app.task(name='vulnerability_scan', queue='orchestrator_queue', bind=True, base=RengineTask)
def vulnerability_scan(self, urls=None, ctx=None, description=None):
    """
        This function will serve as an entrypoint to vulnerability scan.
        All other vulnerability scan will be run from here including nuclei, crlfuzz, etc
    """
    from reNgine.utils.scan_helpers import execute_grouped_tasks

    if urls is None:
        urls = []
    if ctx is None:
        ctx = {}

    logger.info('Running Vulnerability Scan Queue')
    config = TaskConfig(self.yaml_configuration, self.results_dir, self.scan_id, self.filename)
    vuln_config = config.get_config(VULNERABILITY_SCAN) or {}
    should_run_nuclei = vuln_config.get(RUN_NUCLEI, True)
    should_run_crlfuzz = vuln_config.get(RUN_CRLFUZZ, False)
    should_run_dalfox = vuln_config.get(RUN_DALFOX, False)
    should_run_s3scanner = vuln_config.get(RUN_S3SCANNER, True)

    grouped_tasks = []
    if should_run_nuclei:
        _task = nuclei_scan.si(
            urls=urls,
            ctx=ctx,
            description='Nuclei Scan'
        )
        grouped_tasks.append(_task)

    if should_run_crlfuzz:
        _task = crlfuzz_scan.si(
            urls=urls,
            ctx=ctx,
            description='CRLFuzz Scan'
        )
        grouped_tasks.append(_task)

    if should_run_dalfox:
        _task = dalfox_scan.si(
            urls=urls,
            ctx=ctx,
            description='Dalfox XSS Scan'
        )
        grouped_tasks.append(_task)

    if should_run_s3scanner:
        _task = s3scanner.si(
            ctx=ctx,
            description='Misconfigured S3 Buckets Scanner'
        )
        grouped_tasks.append(_task)

    execute_grouped_tasks(
        self, 
        grouped_tasks, 
        task_name="vulnerability_scan", 
        callback_kwargs={'description': 'Processing vulnerability scan results'}
    )
    
    logger.info('Vulnerability scan tasks submitted...')
    return {'status': 'submitted'}


@app.task(name='nuclei_scan', queue='vulnerability_scan_queue', base=RengineTask, bind=True)
def nuclei_scan(self, urls=None, ctx=None, description=None):
    """HTTP vulnerability scan using Nuclei

    Args:
        urls (list, optional): If passed, filter on those URLs.
        description (str, optional): Task description shown in UI.

    Notes:
    Unfurl the urls to keep only domain and path, will be sent to vuln scan and
    ignore certain file extensions. Thanks: https://github.com/six2dez/reconftw
    """
    from reNgine.utils.scan_helpers import execute_grouped_tasks

    if urls is None:
        urls = []
    if ctx is None:
        ctx = {}
    # Initialize task config
    config = TaskConfig(self.yaml_configuration, self.results_dir, self.scan_id, self.filename)
    
    # Get input path
    input_path = config.get_input_path('endpoints_vulnerability_scan')
    
    # Get vulnerability scan config values
    enable_http_crawl = config.get_http_crawl_enabled(VULNERABILITY_SCAN)
    concurrency = config.get_threads(VULNERABILITY_SCAN)
    intensity = config.get_intensity(VULNERABILITY_SCAN)
    rate_limit = config.get_rate_limit(VULNERABILITY_SCAN)
    retries = config.get_value(VULNERABILITY_SCAN, RETRIES, DEFAULT_RETRIES)
    timeout = config.get_timeout(VULNERABILITY_SCAN)
    custom_header = config.prepare_custom_header(VULNERABILITY_SCAN)
    should_fetch_gpt_report = config.get_gpt_report_enabled(VULNERABILITY_SCAN)
    proxy = config.get_proxy()
    nuclei_specific_config = config.get_config(VULNERABILITY_SCAN).get('nuclei', {})
    use_nuclei_conf = nuclei_specific_config.get(USE_NUCLEI_CONFIG, False)
    severities = nuclei_specific_config.get(NUCLEI_SEVERITY, NUCLEI_DEFAULT_SEVERITIES)
    tags = nuclei_specific_config.get(NUCLEI_TAGS, [])
    tags = ','.join(tags)
    nuclei_templates = nuclei_specific_config.get(NUCLEI_TEMPLATE)
    custom_nuclei_templates = nuclei_specific_config.get(NUCLEI_CUSTOM_TEMPLATE)

    # Get alive endpoints
    urls = prepare_urls_with_fallback(
        urls, 
        input_path, 
        ctx,
        is_alive=True,
        ignore_files=True
    )

    if not urls:
        logger.error('No URLs to scan for Nuclei. Skipping.')
        return

    if intensity == 'normal': # reduce number of endpoints to scan
        if not os.path.exists(input_path):
            with open(input_path, 'w') as f:
                f.write('\n'.join(urls))

        unfurl_filter = str(Path(self.results_dir) / 'urls_unfurled.txt')

        # Use CommandBuilder for these shell commands
        cmd_builder_cat = CommandBuilder('cat')
        cmd_builder_cat.add_option(input_path)
        cmd_builder_cat.add_pipe_command('unfurl -u format %s://%d%p')
        cmd_builder_cat.add_pipe_command('uro')
        cmd_builder_cat.add_redirection('>', unfurl_filter)
        
        run_command_line.delay(
            cmd_builder_cat.build_string(),
            shell=True,
            history_file=self.history_file,
            scan_id=self.scan_id,
            activity_id=self.activity_id)
        
        cmd_builder_sort = CommandBuilder('sort')
        cmd_builder_sort.add_option('-u')
        cmd_builder_sort.add_option(unfurl_filter)
        cmd_builder_sort.add_option('-o')
        cmd_builder_sort.add_option(unfurl_filter)
        
        run_command_line.delay(
            cmd_builder_sort.build_string(),
            shell=True,
            history_file=self.history_file,
            scan_id=self.scan_id,
            activity_id=self.activity_id)

        if not os.path.exists(unfurl_filter) or os.path.getsize(unfurl_filter) == 0:
            logger.error(f"Failed to create or empty unfurled URLs file at {unfurl_filter}")
            unfurl_filter = input_path

        input_path = unfurl_filter

    # Build templates
    logger.info('Updating Nuclei templates ...')
    nuclei_builder = CommandBuilder('nuclei')
    nuclei_builder.add_option('-update-templates')
    run_command_line.delay(
        nuclei_builder.build_list(),
        shell=False,
        history_file=self.history_file,
        scan_id=self.scan_id,
        activity_id=self.activity_id
    )
    templates = []
    if not (nuclei_templates or custom_nuclei_templates):
        templates.append(NUCLEI_DEFAULT_TEMPLATES_PATH)

    if nuclei_templates:
        if ALL in nuclei_templates:
            template = NUCLEI_DEFAULT_TEMPLATES_PATH
            templates.append(template)
        else:
            templates.extend(nuclei_templates)

    if custom_nuclei_templates:
        custom_nuclei_template_paths = [
            str(Path(NUCLEI_DEFAULT_TEMPLATES_PATH) / f'{str(elem)}.yaml') 
            for elem in custom_nuclei_templates
        ]
        templates.extend(custom_nuclei_template_paths)

    # Build CMD base (without specific severity)
    cmd_builder = CommandBuilder('nuclei')
    cmd_builder.add_option('-j')

    # Configuration Nuclei
    if use_nuclei_conf:
        config_path = str(Path.home() / '.config' / 'nuclei' / 'config.yaml')
        cmd_builder.add_option('-config', config_path)

    cmd_builder.add_option('-irr')
    cmd_builder.add_option(custom_header, condition=bool(custom_header))
    cmd_builder.add_option('-l', input_path)
    cmd_builder.add_option('-c', str(concurrency), condition=concurrency > 0)
    cmd_builder.add_option('-proxy', proxy, condition=bool(proxy))
    cmd_builder.add_option('-retries', retries, condition=retries > 0)
    cmd_builder.add_option('-rl', rate_limit, condition=rate_limit > 0)
    cmd_builder.add_option('-timeout', str(timeout), condition=timeout and timeout > 0)
    cmd_builder.add_option('-tags', tags, condition=bool(tags))
    cmd_builder.add_option('-silent')

    # Add templates
    for tpl in templates:
        cmd_builder.add_option('-t', tpl)

    # Build the command as a string to pass to nuclei_individual_severity_module
    cmd = cmd_builder.build_string()

    # Execute tasks for each severity in parallel
    grouped_tasks = []
    custom_ctx = ctx
    for severity in severities:
        custom_ctx['track'] = True
        _task = nuclei_individual_severity_module.si(
            cmd,
            severity,
            enable_http_crawl,
            should_fetch_gpt_report,
            ctx=custom_ctx,
            description=f'Nuclei Scan with severity {severity}'
        )
        grouped_tasks.append(_task)

    execute_grouped_tasks(
        self,
        grouped_tasks,
        task_name="nuclei_scan",
        callback_kwargs={"scan_id": self.scan_id}
    )

    logger.info('Vulnerability scan with all severities completed...')

    return None


@app.task(name='nuclei_individual_severity_module', queue='nuclei_queue', base=RengineTask, bind=True)
def nuclei_individual_severity_module(self, cmd, severity, enable_http_crawl, should_fetch_gpt_report, ctx=None, description=None):
    '''
        This celery task will run vulnerability scan in parallel.
        All severities supplied should run in parallel as grouped tasks.
    '''
    from reNgine.utils.db import (
        get_vulnerability_gpt_report,
        record_exists,
        save_vulnerability,
        save_endpoint,
    )
    from reNgine.utils.notifications import send_vulnerability_scan_summary
    
    if ctx is None:
        ctx = {}

    results = []
    logger.info(f'Running vulnerability scan with severity: {severity}')
    
    # Add severity to the command
    # Using CommandBuilder to ensure the formatting is correct
    cmd_parts = shlex.split(cmd)
    cmd_builder = CommandBuilder(cmd_parts[0])
    for i in range(1, len(cmd_parts)):
        cmd_builder.add_raw_option(cmd_parts[i])
    
    cmd_builder.add_option('-severity', severity)
    cmd_final = cmd_builder.build_list()
    
    for line in stream_command(
            cmd_final,
            shell=False,
            history_file=self.history_file,
            scan_id=self.scan_id,
            activity_id=self.activity_id):

        if not isinstance(line, dict):
            continue

        results.append(line)

        # Gather nuclei results
        vuln_data = parse_nuclei_result(line)

        # Get corresponding subdomain
        http_url = sanitize_url(line.get('matched-at'))
        subdomain_name = get_subdomain_from_url(http_url)

        try:
            subdomain = Subdomain.objects.get(
                name=subdomain_name,
                scan_history=self.scan,
                target_domain=self.domain
            )
        except Subdomain.DoesNotExist:
            logger.warning(f'Subdomain {subdomain_name} was not found in the db, skipping vulnerability scan for this subdomain.')
            continue

        # Look for duplicate vulnerabilities by excluding records that might change but are irrelevant.
        object_comparison_exclude = ['request','response', 'curl_command', 'tags', 'references', 'cve_ids', 'cwe_ids']

        # Add subdomain and target domain to the duplicate check
        vuln_data_copy = vuln_data.copy()
        vuln_data_copy['subdomain'] = subdomain
        vuln_data_copy['target_domain'] = self.domain

        # Check if record exists, if exists do not save it
        if record_exists(Vulnerability, data=vuln_data_copy, exclude_keys=object_comparison_exclude):
            logger.warning(f'Nuclei vulnerability of severity {severity} : {vuln_data_copy["name"]} for {subdomain_name} already exists')
            continue

        # Get or create EndPoint object
        response = line.get('response')
        httpx_crawl = False if response else enable_http_crawl # avoid yet another httpx crawl
        endpoint, _ = save_endpoint(
            http_url,
            crawl=httpx_crawl,
            subdomain=subdomain,
            ctx=ctx)
        if endpoint:
            http_url = endpoint.http_url
            if not httpx_crawl:
                output = parse_curl_output(response)
                endpoint.http_status = output['http_status']
                endpoint.save()

        # Get or create Vulnerability object
        vuln, _ = save_vulnerability(
            target_domain=self.domain,
            http_url=http_url,
            scan_history=self.scan,
            subscan=self.subscan,
            subdomain=subdomain,
            **vuln_data)
        if not vuln:
            continue

        # Print vuln
        severity = line['info'].get('severity', 'unknown')
        logger.warning(str(vuln))

        send_vulnerability_notification(
            self, 
            vuln, 
            http_url, 
            subdomain_name, 
            severity
        )

        # Send report to hackerone
        hackerone_query = Hackerone.objects.all()
        if (
            hackerone_query.exists()
            and severity not in ('info', 'low')
            and vuln.target_domain.h1_team_handle
        ):
            hackerone = hackerone_query.first()
            if (
                hackerone.send_critical
                and severity == 'critical'
                or hackerone.send_high
                and severity == 'high'
                or hackerone.send_medium
                and severity == 'medium'
            ):
                send_hackerone_report.delay(vuln.id)

    # Write results to JSON file
    with open(self.output_path, 'w') as f:
        json.dump(results, f, indent=4)

    send_vulnerability_scan_summary(self)

    # after vulnerability scan is done, we need to run gpt if
    # should_fetch_gpt_report and openapi key exists

    if should_fetch_gpt_report and OpenAiAPIKey.objects.all().first():
        logger.info('Getting Vulnerability GPT Report')
        vulns = Vulnerability.objects.filter(
            scan_history__id=self.scan_id
        ).filter(
            source=NUCLEI
        ).exclude(
            severity=0
        )
        # find all unique vulnerabilities based on path and title
        # all unique vulnerability will go thru gpt function and get report
        # once report is got, it will be matched with other vulnerabilities and saved
        unique_vulns = set()
        for vuln in vulns:
            unique_vulns.add((vuln.name, vuln.get_path()))

        unique_vulns = list(unique_vulns)

        with concurrent.futures.ThreadPoolExecutor(max_workers=DEFAULT_THREADS) as executor:
            future_to_gpt = {executor.submit(get_vulnerability_gpt_report, vuln): vuln for vuln in unique_vulns}

            # Wait for all tasks to complete
            for future in concurrent.futures.as_completed(future_to_gpt):
                future_to_gpt[future]
                try:
                    future.result()
                except Exception as e:
                    logger.error(f"Exception for Vulnerability {vuln}: {e}")

        return None

@app.task(name='dalfox_scan', queue='io_queue', base=RengineTask, bind=True)
def dalfox_scan(self, urls=None, ctx=None, description=None):
    from reNgine.utils.notifications import send_vulnerability_notification, send_vulnerability_scan_summary
    
    # Initialize task config
    config = TaskConfig(self.yaml_configuration, self.results_dir, self.scan_id, self.filename)
    
    # Get basic config
    vuln_config = config.get_config(VULNERABILITY_SCAN)
    should_fetch_gpt_report = config.get_gpt_report_enabled(VULNERABILITY_SCAN)
    
    # Get dalfox specific config
    dalfox_config = vuln_config.get(DALFOX) or {}
    custom_header = config.prepare_custom_header(VULNERABILITY_SCAN, 'dalfox')
    proxy = config.get_proxy()
    is_waf_evasion = dalfox_config.get(WAF_EVASION, False)
    blind_xss_server = dalfox_config.get(BLIND_XSS_SERVER)
    user_agent = dalfox_config.get(USER_AGENT) or config.yaml_configuration.get(USER_AGENT)
    timeout = dalfox_config.get(TIMEOUT) or config.get_timeout(VULNERABILITY_SCAN)
    delay = dalfox_config.get(DELAY)
    threads = dalfox_config.get(THREADS) or config.get_threads(VULNERABILITY_SCAN)
    input_path = config.get_input_path('endpoints_dalfox')
    
    urls = prepare_urls_with_fallback(
        urls, 
        input_path, 
        ctx,
        is_alive=True,
        ignore_files=False
    )

    if not urls:
        logger.error('No URLs to scan for XSS. Skipping.')
        return

    # command builder
    cmd_builder = CommandBuilder('dalfox')
    cmd_builder.add_option('--silence')
    cmd_builder.add_option('--no-color')
    cmd_builder.add_option('--no-spinner')
    cmd_builder.add_option('--only-poc', 'r')
    cmd_builder.add_option('--ignore-return', '302,404,403')
    cmd_builder.add_option('--skip-bav')
    cmd_builder.add_option('file', input_path)
    cmd_builder.add_option('--proxy', proxy, condition=bool(proxy))
    cmd_builder.add_option('--waf-evasion', condition=is_waf_evasion)
    cmd_builder.add_option('-b', blind_xss_server, condition=bool(blind_xss_server))
    cmd_builder.add_option('--delay', delay, condition=bool(delay))
    cmd_builder.add_option('--timeout', timeout, condition=bool(timeout))
    cmd_builder.add_option('--user-agent', user_agent, condition=bool(user_agent))
    cmd_builder.add_option(custom_header, condition=bool(custom_header))
    cmd_builder.add_option('--worker', threads, condition=bool(threads))
    cmd_builder.add_option('--format', 'json')

    results = []
    for line in stream_command(
            cmd_builder.build_list(),
            history_file=self.history_file,
            scan_id=self.scan_id,
            activity_id=self.activity_id,
            trunc_char=','
        ):
        if not isinstance(line, dict):
            continue

        results.append(line)

        vuln_data = parse_dalfox_result(line)

        http_url = sanitize_url(line.get('data'))
        subdomain_name = get_subdomain_from_url(http_url)

        try:
            subdomain = Subdomain.objects.get(
                name=subdomain_name,
                scan_history=self.scan,
                target_domain=self.domain
            )
        except Subdomain.DoesNotExist:
            logger.warning(f'Subdomain {subdomain_name} was not found in the db, skipping dalfox scan for this subdomain.')
            continue

        endpoint, _ = save_endpoint(
            http_url,
            crawl=True,
            subdomain=subdomain,
            ctx=ctx
        )
        if endpoint:
            http_url = endpoint.http_url
            endpoint.save()

        vuln, _ = save_vulnerability(
            target_domain=self.domain,
            http_url=http_url,
            scan_history=self.scan,
            subscan=self.subscan,
            **vuln_data
        )

        if not vuln:
            continue

        # Determination of the severity
        severity = {
            "1": "low",
            "2": "medium", 
            "3": "high",
            "4": "critical"
        }.get(str(vuln.severity), "medium")
        
        # Sending the notification
        send_vulnerability_notification(
            self, 
            vuln, 
            http_url, 
            subdomain_name,
            severity
        )

    # after vulnerability scan is done, we need to run gpt if
    # should_fetch_gpt_report and openapi key exists

    if should_fetch_gpt_report and OpenAiAPIKey.objects.all().first():
        logger.info('Getting Dalfox Vulnerability GPT Report')
        vulns = Vulnerability.objects.filter(
            scan_history__id=self.scan_id
        ).filter(
            source=DALFOX
        ).exclude(
            severity=0
        )

        _vulns = []
        for vuln in vulns:
            _vulns.append((vuln.name, vuln.http_url))

        with concurrent.futures.ThreadPoolExecutor(max_workers=DEFAULT_THREADS) as executor:
            future_to_gpt = {executor.submit(get_vulnerability_gpt_report, vuln): vuln for vuln in _vulns}

            # Wait for all tasks to complete
            for future in concurrent.futures.as_completed(future_to_gpt):
                future_to_gpt[future]
                try:
                    future.result()
                except Exception as e:
                    logger.error(f"Exception for Vulnerability {vuln}: {e}")

    send_vulnerability_scan_summary(self)

    return results


@app.task(name='crlfuzz_scan', queue='io_queue', base=RengineTask, bind=True)
def crlfuzz_scan(self, urls=None, ctx=None, description=None):
    """CRLF Fuzzing with CRLFuzz

    Args:
        urls (list, optional): If passed, filter on those URLs.
        description (str, optional): Task description shown in UI.
    """
    from reNgine.utils.db import (
        get_vulnerability_gpt_report,
        save_vulnerability,
        save_endpoint,
    )
    from reNgine.utils.notifications import send_vulnerability_notification, send_vulnerability_scan_summary
    from reNgine.utils.command_builder import generate_header_param

    # Initialize task config
    config = TaskConfig(self.yaml_configuration, self.results_dir, self.scan_id, self.filename)

    # Get basic config
    vuln_config = config.get_config(VULNERABILITY_SCAN)
    should_fetch_gpt_report = config.get_gpt_report_enabled(VULNERABILITY_SCAN)

    # Get crlfuzz specific config
    custom_header = config.prepare_custom_header(VULNERABILITY_SCAN, 'crlfuzz')
    proxy = config.get_proxy()
    user_agent = vuln_config.get(USER_AGENT) or config.yaml_configuration.get(USER_AGENT)
    threads = config.get_threads(VULNERABILITY_SCAN)
    input_path = config.get_input_path('endpoints_crlf')
    output_path = config.get_output_path()

    if urls is None:
        urls = []
    if ctx is None:
        ctx = {}

    urls = prepare_urls_with_fallback(
        urls, 
        input_path, 
        ctx,
        is_alive=True,
        ignore_files=False
    )

    if not urls:
        logger.error('No URLs to scan for CRLF. Skipping.')
        return

    # command builder
    crlfuzz_builder = CommandBuilder('crlfuzz')
    crlfuzz_builder.add_option('-s')
    crlfuzz_builder.add_option('-l', input_path)
    crlfuzz_builder.add_option('-x', proxy, bool(proxy))

    # Add user-agent as header if specified
    if user_agent:
        crlfuzz_builder.add_option('-H', f'User-Agent: {user_agent}')

    if header_param := generate_header_param(custom_header, 'crlfuzz'):
        crlfuzz_builder.add_option(header_param)

    crlfuzz_builder.add_option('-o', output_path)
    crlfuzz_builder.add_option('-c', threads, bool(threads))

    run_command_line.delay(
        crlfuzz_builder.build_list(),
        history_file=self.history_file,
        scan_id=self.scan_id,
        activity_id=self.activity_id
    )

    if not os.path.isfile(output_path):
        logger.info('No Results from CRLFuzz')
        return

    crlfs = []
    with open(output_path, 'r') as file:
        crlfs = file.readlines()

    for crlf in crlfs:
        url = crlf.strip()

        vuln_data = parse_crlfuzz_result(url)

        http_url = sanitize_url(url)
        subdomain_name = get_subdomain_from_url(http_url)

        try:
            subdomain = Subdomain.objects.get(
                name=subdomain_name,
                scan_history=self.scan,
                target_domain=self.domain
            )
        except Subdomain.DoesNotExist:
            logger.warning(f'Subdomain {subdomain_name} was not found in the db, skipping crlfuzz scan for this subdomain.')
            continue

        endpoint, _ = save_endpoint(
            http_url,
            crawl=True,
            subdomain=subdomain,
            ctx=ctx
        )
        if endpoint:
            http_url = endpoint.http_url
            endpoint.save()

        vuln, _ = save_vulnerability(
            target_domain=self.domain,
            http_url=http_url,
            scan_history=self.scan,
            subscan=self.subscan,
            **vuln_data
        )

        if not vuln:
            continue

        # CRLF is generally considered medium
        severity = "medium"

        # Sending the notification
        send_vulnerability_notification(
            self, 
            vuln, 
            http_url, 
            subdomain_name,
            severity
        )

    # GPT report handling
    if should_fetch_gpt_report and OpenAiAPIKey.objects.all().first():
        logger.info('Getting CRLFuzz Vulnerability GPT Report')
        vulns = Vulnerability.objects.filter(
            scan_history__id=self.scan_id
        ).filter(
            source=CRLFUZZ
        ).exclude(
            severity=0
        )

        _vulns = []
        for vuln in vulns:
            _vulns.append((vuln.name, vuln.http_url))

        with concurrent.futures.ThreadPoolExecutor(max_workers=DEFAULT_THREADS) as executor:
            future_to_gpt = {executor.submit(get_vulnerability_gpt_report, vuln): vuln for vuln in _vulns}

            # Wait for all tasks to complete
            for future in concurrent.futures.as_completed(future_to_gpt):
                future_to_gpt[future]
                try:
                    future.result()
                except Exception as e:
                    logger.error(f"Exception for Vulnerability {vuln}: {e}")

    send_vulnerability_scan_summary(self)

    return []

@app.task(name='s3scanner', queue='io_queue', base=RengineTask, bind=True)
def s3scanner(self, ctx=None, description=None):
    """Bucket Scanner

    Args:
        ctx (dict): Context
        description (str, optional): Task description shown in UI.
    """
    if ctx is None:
        ctx = {}
    input_path = str(Path(self.results_dir) / f'{self.scan_id}_s3_bucket_discovery.txt')

    subdomains = Subdomain.objects.filter(scan_history=self.scan)
    if not subdomains:
        logger.error('No subdomains found for S3Scanner. Skipping.')
        return

    with open(input_path, 'w') as f:
        for subdomain in subdomains:
            f.write(subdomain.name + '\n')

    config = TaskConfig(self.yaml_configuration, self.results_dir, self.scan_id, self.filename)
    vuln_config = config.get_value(S3SCANNER) or {}
    threads = config.get_threads(VULNERABILITY_SCAN)
    providers = vuln_config.get(PROVIDERS, S3SCANNER_DEFAULT_PROVIDERS)
    scan_history = ScanHistory.objects.filter(pk=self.scan_id).first()
    for provider in providers:
        cmd_builder = CommandBuilder('s3scanner')
        cmd_builder.add_option('-bucket-file', input_path)
        cmd_builder.add_option('-enumerate')
        cmd_builder.add_option('-provider', provider)
        cmd_builder.add_option('-threads', threads)
        cmd_builder.add_option('-json')

        for line in stream_command(
                cmd_builder.build_list(),
                shell=False,
                history_file=self.history_file,
                scan_id=self.scan_id,
                activity_id=self.activity_id):

            if not isinstance(line, dict):
                continue

            if line.get('bucket', {}).get('exists', 0) == 1:
                result = parse_s3scanner_result(line)
                s3bucket, created = S3Bucket.objects.get_or_create(**result)
                scan_history.buckets.add(s3bucket)
                logger.info(f"s3 bucket added {result['provider']}-{result['name']}-{result['region']}")
