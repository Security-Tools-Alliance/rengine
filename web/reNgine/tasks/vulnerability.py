import json
import os
import time
import concurrent.futures

from pathlib import Path
from celery import group

from reNgine.definitions import (
    ALL,
    BLIND_XSS_SERVER,
    CRLFUZZ,
    DALFOX,
    DEFAULT_SCAN_INTENSITY,
    DELAY,
    FETCH_GPT_REPORT,
    INTENSITY,
    NUCLEI,
    NUCLEI_CONCURRENCY,
    NUCLEI_CUSTOM_TEMPLATE,
    NUCLEI_DEFAULT_SEVERITIES,
    NUCLEI_DEFAULT_TEMPLATES_PATH,
    NUCLEI_SEVERITY,
    NUCLEI_TAGS,
    NUCLEI_TEMPLATE,
    PROVIDERS,
    RATE_LIMIT,
    RETRIES,
    RUN_CRLFUZZ,
    RUN_DALFOX,
    RUN_NUCLEI,
    RUN_S3SCANNER,
    S3SCANNER,
    S3SCANNER_DEFAULT_PROVIDERS,
    THREADS,
    TIMEOUT,
    USE_NUCLEI_CONFIG,
    USER_AGENT,
    CUSTOM_HEADER,
    VULNERABILITY_SCAN,
    WAF_EVASION,
)
from reNgine.settings import (
    DEFAULT_GET_GPT_REPORT,
    DEFAULT_HTTP_TIMEOUT,
    DEFAULT_RATE_LIMIT,
    DEFAULT_RETRIES,
    DEFAULT_THREADS,
)
from reNgine.celery import app
from reNgine.celery_custom_task import RengineTask
from reNgine.tasks.notification import send_hackerone_report
from reNgine.tasks.command import run_command_line
from reNgine.utils.logger import Logger
from reNgine.utils.formatters import (
    generate_header_param
)
from reNgine.utils.command_executor import (
    stream_command
)

from reNgine.utils.http import (
    get_http_crawl_value,
    get_subdomain_from_url,
    parse_curl_output,
    sanitize_url,
)
from reNgine.utils.parsers import (
    parse_s3scanner_result,
    parse_nuclei_result,
    parse_dalfox_result,
    parse_crlfuzz_result,
)
from reNgine.utils.utils import is_iterable
from scanEngine.models import (
    Hackerone,
    Notification,
)
from startScan.models import (
    S3Bucket,
    ScanHistory,
    Subdomain,
    Vulnerability,
)
from dashboard.models import OpenAiAPIKey
logger = Logger(True)

@app.task(name='vulnerability_scan', queue='vulnerability_scan_queue', bind=True, base=RengineTask)
def vulnerability_scan(self, urls=None, ctx=None, description=None):
    """
        This function will serve as an entrypoint to vulnerability scan.
        All other vulnerability scan will be run from here including nuclei, crlfuzz, etc
    """
    if urls is None:
        urls = []
    if ctx is None:
        ctx = {}

    logger.info('Running Vulnerability Scan Queue')
    config = self.yaml_configuration.get(VULNERABILITY_SCAN) or {}
    should_run_nuclei = config.get(RUN_NUCLEI, True)
    should_run_crlfuzz = config.get(RUN_CRLFUZZ, False)
    should_run_dalfox = config.get(RUN_DALFOX, False)
    should_run_s3scanner = config.get(RUN_S3SCANNER, True)

    grouped_tasks = []
    if should_run_nuclei:
        _task = nuclei_scan.si(
            urls=urls,
            ctx=ctx,
            description='Nuclei Scan'
        )
        grouped_tasks.append(_task)

    if should_run_crlfuzz:
        _task = crlfuzz_scan.si(
            urls=urls,
            ctx=ctx,
            description='CRLFuzz Scan'
        )
        grouped_tasks.append(_task)

    if should_run_dalfox:
        _task = dalfox_xss_scan.si(
            urls=urls,
            ctx=ctx,
            description='Dalfox XSS Scan'
        )
        grouped_tasks.append(_task)

    if should_run_s3scanner:
        _task = s3scanner.si(
            ctx=ctx,
            description='Misconfigured S3 Buckets Scanner'
        )
        grouped_tasks.append(_task)

    celery_group = group(grouped_tasks)
    job = celery_group.apply_async()

    while not job.ready():
        # wait for all jobs to complete
        time.sleep(5)

    logger.info('Vulnerability scan completed...')

    # return results
    return None


@app.task(name='nuclei_scan', queue='vulnerability_scan_queue', base=RengineTask, bind=True)
def nuclei_scan(self, urls=None, ctx=None, description=None):
    """HTTP vulnerability scan using Nuclei

    Args:
        urls (list, optional): If passed, filter on those URLs.
        description (str, optional): Task description shown in UI.

    Notes:
    Unfurl the urls to keep only domain and path, will be sent to vuln scan and
    ignore certain file extensions. Thanks: https://github.com/six2dez/reconftw
    """
    from reNgine.utils.db import get_http_urls, get_random_proxy
 
    if urls is None:
        urls = []
    if ctx is None:
        ctx = {}
    # Config
    config = self.yaml_configuration.get(VULNERABILITY_SCAN) or {}
    input_path = str(Path(self.results_dir) / 'input_endpoints_vulnerability_scan.txt')
    enable_http_crawl = get_http_crawl_value(self, config)
    concurrency = config.get(NUCLEI_CONCURRENCY) or self.yaml_configuration.get(THREADS, DEFAULT_THREADS)
    intensity = config.get(INTENSITY) or self.yaml_configuration.get(INTENSITY, DEFAULT_SCAN_INTENSITY)
    rate_limit = config.get(RATE_LIMIT) or self.yaml_configuration.get(RATE_LIMIT, DEFAULT_RATE_LIMIT)
    retries = config.get(RETRIES) or self.yaml_configuration.get(RETRIES, DEFAULT_RETRIES)
    timeout = config.get(TIMEOUT) or self.yaml_configuration.get(TIMEOUT, DEFAULT_HTTP_TIMEOUT)
    custom_header = config.get(CUSTOM_HEADER) or self.yaml_configuration.get(CUSTOM_HEADER)
    if custom_header:
        custom_header = generate_header_param(custom_header, 'common')
    should_fetch_gpt_report = config.get(FETCH_GPT_REPORT, DEFAULT_GET_GPT_REPORT)
    proxy = get_random_proxy()
    nuclei_specific_config = config.get('nuclei', {})
    use_nuclei_conf = nuclei_specific_config.get(USE_NUCLEI_CONFIG, False)
    severities = nuclei_specific_config.get(NUCLEI_SEVERITY, NUCLEI_DEFAULT_SEVERITIES)
    tags = nuclei_specific_config.get(NUCLEI_TAGS, [])
    tags = ','.join(tags)
    nuclei_templates = nuclei_specific_config.get(NUCLEI_TEMPLATE)
    custom_nuclei_templates = nuclei_specific_config.get(NUCLEI_CUSTOM_TEMPLATE)
    # severities_str = ','.join(severities)

    # Get alive endpoints
    if urls and is_iterable(urls):
        with open(input_path, 'w') as f:
            f.write('\n'.join(urls))
    else:
        logger.debug('Getting alive endpoints for Nuclei scan')
        urls = get_http_urls(
            is_alive=True,
            ignore_files=True,
            write_filepath=input_path,
            ctx=ctx
        )

    if not urls:
        logger.error('No URLs to scan for Nuclei. Skipping.')
        return

    if intensity == 'normal': # reduce number of endpoints to scan
        if not os.path.exists(input_path):
            with open(input_path, 'w') as f:
                f.write('\n'.join(urls))

        unfurl_filter = str(Path(self.results_dir) / 'urls_unfurled.txt')

        run_command_line.delay(
            f"cat {input_path} | unfurl -u format %s://%d%p |uro > {unfurl_filter}",
            shell=True,
            history_file=self.history_file,
            scan_id=self.scan_id,
            activity_id=self.activity_id)
        run_command_line.delay(
            f'sort -u {unfurl_filter} -o {unfurl_filter}',
            shell=True,
            history_file=self.history_file,
            scan_id=self.scan_id,
            activity_id=self.activity_id)

        if not os.path.exists(unfurl_filter) or os.path.getsize(unfurl_filter) == 0:
            logger.error(f"Failed to create or empty unfurled URLs file at {unfurl_filter}")
            unfurl_filter = input_path

        input_path = unfurl_filter

    # Build templates
    logger.info('Updating Nuclei templates ...')
    run_command_line.delay(
        'nuclei -update-templates',
        shell=True,
        history_file=self.history_file,
        scan_id=self.scan_id,
        activity_id=self.activity_id)
    templates = []
    if not (nuclei_templates or custom_nuclei_templates):
        templates.append(NUCLEI_DEFAULT_TEMPLATES_PATH)

    if nuclei_templates:
        if ALL in nuclei_templates:
            template = NUCLEI_DEFAULT_TEMPLATES_PATH
            templates.append(template)
        else:
            templates.extend(nuclei_templates)

    if custom_nuclei_templates:
        custom_nuclei_template_paths = [
            str(Path(NUCLEI_DEFAULT_TEMPLATES_PATH) / f'{str(elem)}.yaml') 
            for elem in custom_nuclei_templates
        ]
        templates.extend(custom_nuclei_template_paths)

    # Build CMD
    cmd = 'nuclei -j'
    cmd += (' -config ' + str(Path.home() / '.config' / 'nuclei' / 'config.yaml')) if use_nuclei_conf else ''
    cmd += ' -irr'
    cmd += f' {custom_header}' if custom_header else ''
    cmd += f' -l {input_path}'
    cmd += f' -c {str(concurrency)}' if concurrency > 0 else ''
    cmd += f' -proxy {proxy} ' if proxy else ''
    cmd += f' -retries {retries}' if retries > 0 else ''
    cmd += f' -rl {rate_limit}' if rate_limit > 0 else ''
    # cmd += f' -severity {severities_str}'
    cmd += f' -timeout {str(timeout)}' if timeout and timeout > 0 else ''
    cmd += f' -tags {tags}' if tags else ''
    cmd += ' -silent'
    for tpl in templates:
        cmd += f' -t {tpl}'


    grouped_tasks = []
    custom_ctx = ctx
    for severity in severities:
        custom_ctx['track'] = True
        _task = nuclei_individual_severity_module.si(
            cmd,
            severity,
            enable_http_crawl,
            should_fetch_gpt_report,
            ctx=custom_ctx,
            description=f'Nuclei Scan with severity {severity}'
        )
        grouped_tasks.append(_task)

    celery_group = group(grouped_tasks)
    job = celery_group.apply_async()

    while not job.ready():
        # wait for all jobs to complete
        time.sleep(5)

    logger.info('Vulnerability scan with all severities completed...')

    return None

@app.task(name='nuclei_individual_severity_module', queue='nuclei_queue', base=RengineTask, bind=True)
def nuclei_individual_severity_module(self, cmd, severity, enable_http_crawl, should_fetch_gpt_report, ctx=None, description=None):
    '''
        This celery task will run vulnerability scan in parallel.
        All severities supplied should run in parallel as grouped tasks.
    '''
    from reNgine.utils.db import (
        get_vulnerability_gpt_report,
        record_exists,
        save_vulnerability,
        save_endpoint,
    )
    
    if ctx is None:
        ctx = {}

    results = []
    logger.info(f'Running vulnerability scan with severity: {severity}')
    cmd += f' -severity {severity}'
    # Send start notification
    notif = Notification.objects.first()
    send_status = notif.send_scan_status_notif if notif else False

    for line in stream_command(
            cmd,
            history_file=self.history_file,
            scan_id=self.scan_id,
            activity_id=self.activity_id):

        if not isinstance(line, dict):
            continue

        results.append(line)

        # Gather nuclei results
        vuln_data = parse_nuclei_result(line)

        # Get corresponding subdomain
        http_url = sanitize_url(line.get('matched-at'))
        subdomain_name = get_subdomain_from_url(http_url)

        try:
            subdomain = Subdomain.objects.get(
                name=subdomain_name,
                scan_history=self.scan,
                target_domain=self.domain
            )
        except Subdomain.DoesNotExist:
            logger.warning(f'Subdomain {subdomain_name} was not found in the db, skipping vulnerability scan for this subdomain.')
            continue

        # Look for duplicate vulnerabilities by excluding records that might change but are irrelevant.
        object_comparison_exclude = ['request','response', 'curl_command', 'tags', 'references', 'cve_ids', 'cwe_ids']

        # Add subdomain and target domain to the duplicate check
        vuln_data_copy = vuln_data.copy()
        vuln_data_copy['subdomain'] = subdomain
        vuln_data_copy['target_domain'] = self.domain

        # Check if record exists, if exists do not save it
        if record_exists(Vulnerability, data=vuln_data_copy, exclude_keys=object_comparison_exclude):
            logger.warning(f'Nuclei vulnerability of severity {severity} : {vuln_data_copy["name"]} for {subdomain_name} already exists')
            continue

        # Get or create EndPoint object
        response = line.get('response')
        httpx_crawl = False if response else enable_http_crawl # avoid yet another httpx crawl
        endpoint, _ = save_endpoint(
            http_url,
            crawl=httpx_crawl,
            subdomain=subdomain,
            ctx=ctx)
        if endpoint:
            http_url = endpoint.http_url
            if not httpx_crawl:
                output = parse_curl_output(response)
                endpoint.http_status = output['http_status']
                endpoint.save()

        # Get or create Vulnerability object
        vuln, _ = save_vulnerability(
            target_domain=self.domain,
            http_url=http_url,
            scan_history=self.scan,
            subscan=self.subscan,
            subdomain=subdomain,
            **vuln_data)
        if not vuln:
            continue

        # Print vuln
        severity = line['info'].get('severity', 'unknown')
        logger.warning(str(vuln))


        if (
            notif
            and notif.send_vuln_notif
            and vuln
            and severity in ['low', 'medium', 'high', 'critical']
        ):
            fields = {
                'Severity': f'**{severity.upper()}**',
                'URL': http_url,
                'Subdomain': subdomain_name,
                'Name': vuln.name,
                'Type': vuln.type,
                'Description': vuln.description,
                'Template': vuln.template_url,
                'Tags': vuln.get_tags_str(),
                'CVEs': vuln.get_cve_str(),
                'CWEs': vuln.get_cwe_str(),
                'References': vuln.get_refs_str()
            }
            severity_map = {
                'low': 'info',
                'medium': 'warning',
                'high': 'error',
                'critical': 'error'
            }
            self.notify(
                f'vulnerability_scan_#{vuln.id}',
                severity_map[severity],
                fields,
                add_meta_info=False)

        # Send report to hackerone
        hackerone_query = Hackerone.objects.all()
        if (
            hackerone_query.exists()
            and severity not in ('info', 'low')
            and vuln.target_domain.h1_team_handle
        ):
            hackerone = hackerone_query.first()
            if (
                hackerone.send_critical
                and severity == 'critical'
                or hackerone.send_high
                and severity == 'high'
                or hackerone.send_medium
                and severity == 'medium'
            ):
                send_hackerone_report.delay(vuln.id)

    # Write results to JSON file
    with open(self.output_path, 'w') as f:
        json.dump(results, f, indent=4)

    # Send finish notif
    if send_status:
        vulns = Vulnerability.objects.filter(scan_history__id=self.scan_id)
        info_count = vulns.filter(severity=0).count()
        low_count = vulns.filter(severity=1).count()
        medium_count = vulns.filter(severity=2).count()
        high_count = vulns.filter(severity=3).count()
        critical_count = vulns.filter(severity=4).count()
        unknown_count = vulns.filter(severity=-1).count()
        vulnerability_count = info_count + low_count + medium_count + high_count + critical_count + unknown_count
        fields = {
            'Total': vulnerability_count,
            'Critical': critical_count,
            'High': high_count,
            'Medium': medium_count,
            'Low': low_count,
            'Info': info_count,
            'Unknown': unknown_count
        }
        self.notify(fields=fields)

    # after vulnerability scan is done, we need to run gpt if
    # should_fetch_gpt_report and openapi key exists

    if should_fetch_gpt_report and OpenAiAPIKey.objects.all().first():
        logger.info('Getting Vulnerability GPT Report')
        vulns = Vulnerability.objects.filter(
            scan_history__id=self.scan_id
        ).filter(
            source=NUCLEI
        ).exclude(
            severity=0
        )
        # find all unique vulnerabilities based on path and title
        # all unique vulnerability will go thru gpt function and get report
        # once report is got, it will be matched with other vulnerabilities and saved
        unique_vulns = set()
        for vuln in vulns:
            unique_vulns.add((vuln.name, vuln.get_path()))

        unique_vulns = list(unique_vulns)

        with concurrent.futures.ThreadPoolExecutor(max_workers=DEFAULT_THREADS) as executor:
            future_to_gpt = {executor.submit(get_vulnerability_gpt_report, vuln): vuln for vuln in unique_vulns}

            # Wait for all tasks to complete
            for future in concurrent.futures.as_completed(future_to_gpt):
                gpt = future_to_gpt[future]
                try:
                    future.result()
                except Exception as e:
                    logger.error(f"Exception for Vulnerability {vuln}: {e}")

        return None

@app.task(name='dalfox_xss_scan', queue='dalfox_queue', base=RengineTask, bind=True)
def dalfox_xss_scan(self, urls=None, ctx=None, description=None):
    """XSS Scan using dalfox

    Args:
        urls (list, optional): If passed, filter on those URLs.
        description (str, optional): Task description shown in UI.
    """
    from reNgine.utils.db import (
        get_http_urls,
        get_random_proxy,
        get_vulnerability_gpt_report,
        save_vulnerability,
        save_endpoint,
    )

    if urls is None:
        urls = []
    if ctx is None:
        ctx = {}
    vuln_config = self.yaml_configuration.get(VULNERABILITY_SCAN) or {}
    should_fetch_gpt_report = vuln_config.get(FETCH_GPT_REPORT, DEFAULT_GET_GPT_REPORT)
    dalfox_config = vuln_config.get(DALFOX) or {}
    custom_header = dalfox_config.get(CUSTOM_HEADER) or self.yaml_configuration.get(CUSTOM_HEADER)
    if custom_header:
        custom_header = generate_header_param(custom_header, 'dalfox')
    proxy = get_random_proxy()
    is_waf_evasion = dalfox_config.get(WAF_EVASION, False)
    blind_xss_server = dalfox_config.get(BLIND_XSS_SERVER)
    user_agent = dalfox_config.get(USER_AGENT) or self.yaml_configuration.get(USER_AGENT)
    timeout = dalfox_config.get(TIMEOUT)
    delay = dalfox_config.get(DELAY)
    threads = dalfox_config.get(THREADS) or self.yaml_configuration.get(THREADS, DEFAULT_THREADS)
    input_path = str(Path(self.results_dir) / 'input_endpoints_dalfox_xss.txt')

    if urls and is_iterable(urls):
        with open(input_path, 'w') as f:
            f.write('\n'.join(urls))
    else:
        urls = get_http_urls(
            is_alive=True,
            ignore_files=False,
            write_filepath=input_path,
            ctx=ctx
        )

    if not urls:
        logger.error('No URLs to scan for XSS. Skipping.')
        return

    notif = Notification.objects.first()
    send_status = notif.send_scan_status_notif if notif else False

    # command builder
    cmd = 'dalfox'
    cmd += ' --silence --no-color --no-spinner'
    cmd += ' --only-poc r '
    cmd += ' --ignore-return 302,404,403'
    cmd += ' --skip-bav'
    cmd += f' file {input_path}'
    cmd += f' --proxy {proxy}' if proxy else ''
    cmd += ' --waf-evasion' if is_waf_evasion else ''
    cmd += f' -b {blind_xss_server}' if blind_xss_server else ''
    cmd += f' --delay {delay}' if delay else ''
    cmd += f' --timeout {timeout}' if timeout else ''
    cmd += f' --user-agent {user_agent}' if user_agent else ''
    cmd += f' {custom_header}' if custom_header else ''
    cmd += f' --worker {threads}' if threads else ''
    cmd += ' --format json'

    results = []
    for line in stream_command(
            cmd,
            history_file=self.history_file,
            scan_id=self.scan_id,
            activity_id=self.activity_id,
            trunc_char=','
        ):
        if not isinstance(line, dict):
            continue

        results.append(line)

        vuln_data = parse_dalfox_result(line)

        http_url = sanitize_url(line.get('data'))
        subdomain_name = get_subdomain_from_url(http_url)

        try:
            subdomain = Subdomain.objects.get(
                name=subdomain_name,
                scan_history=self.scan,
                target_domain=self.domain
            )
        except Subdomain.DoesNotExist:
            logger.warning(f'Subdomain {subdomain_name} was not found in the db, skipping dalfox scan for this subdomain.')
            continue

        endpoint, _ = save_endpoint(
            http_url,
            crawl=True,
            subdomain=subdomain,
            ctx=ctx
        )
        if endpoint:
            http_url = endpoint.http_url
            endpoint.save()

        vuln, _ = save_vulnerability(
            target_domain=self.domain,
            http_url=http_url,
            scan_history=self.scan,
            subscan=self.subscan,
            **vuln_data
        )

        if not vuln:
            continue

    # after vulnerability scan is done, we need to run gpt if
    # should_fetch_gpt_report and openapi key exists

    if should_fetch_gpt_report and OpenAiAPIKey.objects.all().first():
        logger.info('Getting Dalfox Vulnerability GPT Report')
        vulns = Vulnerability.objects.filter(
            scan_history__id=self.scan_id
        ).filter(
            source=DALFOX
        ).exclude(
            severity=0
        )

        _vulns = []
        for vuln in vulns:
            _vulns.append((vuln.name, vuln.http_url))

        with concurrent.futures.ThreadPoolExecutor(max_workers=DEFAULT_THREADS) as executor:
            future_to_gpt = {executor.submit(get_vulnerability_gpt_report, vuln): vuln for vuln in _vulns}

            # Wait for all tasks to complete
            for future in concurrent.futures.as_completed(future_to_gpt):
                gpt = future_to_gpt[future]
                try:
                    future.result()
                except Exception as e:
                    logger.error(f"Exception for Vulnerability {vuln}: {e}")
    return results


@app.task(name='crlfuzz_scan', queue='crlfuzz_queue', base=RengineTask, bind=True)
def crlfuzz_scan(self, urls=None, ctx=None, description=None):
    """CRLF Fuzzing with CRLFuzz

    Args:
        urls (list, optional): If passed, filter on those URLs.
        description (str, optional): Task description shown in UI.
    """
    from reNgine.utils.db import (
        get_http_urls,
        get_random_proxy,
        get_vulnerability_gpt_report,
        save_vulnerability,
        save_endpoint,
    ) 

    if urls is None:
        urls = []
    if ctx is None:
        ctx = {}
    vuln_config = self.yaml_configuration.get(VULNERABILITY_SCAN) or {}
    should_fetch_gpt_report = vuln_config.get(FETCH_GPT_REPORT, DEFAULT_GET_GPT_REPORT)
    custom_header = vuln_config.get(CUSTOM_HEADER) or self.yaml_configuration.get(CUSTOM_HEADER)
    if custom_header:
        custom_header = generate_header_param(custom_header, 'common')
    proxy = get_random_proxy()
    user_agent = vuln_config.get(USER_AGENT) or self.yaml_configuration.get(USER_AGENT)
    threads = vuln_config.get(THREADS) or self.yaml_configuration.get(THREADS, DEFAULT_THREADS)
    input_path = str(Path(self.results_dir) / 'input_endpoints_crlf.txt')
    output_path = str(Path(self.results_dir) / f'{self.filename}')

    if urls and is_iterable(urls):
        with open(input_path, 'w') as f:
            f.write('\n'.join(urls))
    else:
        urls = get_http_urls(
            is_alive=True,
            ignore_files=True,
            write_filepath=input_path,
            ctx=ctx
        )

    if not urls:
        logger.error('No URLs to scan for CRLF. Skipping.')
        return

    notif = Notification.objects.first()
    send_status = notif.send_scan_status_notif if notif else False

    # command builder
    cmd = 'crlfuzz -s'
    cmd += f' -l {input_path}'
    cmd += f' -x {proxy}' if proxy else ''
    cmd += f' {custom_header}' if custom_header else ''
    cmd += f' -o {output_path}'
    cmd += f' -c {threads}' if threads else ''

    run_command_line.delay(
        cmd,
        history_file=self.history_file,
        scan_id=self.scan_id,
        activity_id=self.activity_id
    )

    if not os.path.isfile(output_path):
        logger.info('No Results from CRLFuzz')
        return

    crlfs = []
    with open(output_path, 'r') as file:
        crlfs = file.readlines()

    for crlf in crlfs:
        url = crlf.strip()

        vuln_data = parse_crlfuzz_result(url)

        http_url = sanitize_url(url)
        subdomain_name = get_subdomain_from_url(http_url)

        try:
            subdomain = Subdomain.objects.get(
                name=subdomain_name,
                scan_history=self.scan,
                target_domain=self.domain
            )
        except Subdomain.DoesNotExist:
            logger.warning(f'Subdomain {subdomain_name} was not found in the db, skipping crlfuzz scan for this subdomain.')
            continue

        endpoint, _ = save_endpoint(
            http_url,
            crawl=True,
            subdomain=subdomain,
            ctx=ctx
        )
        if endpoint:
            http_url = endpoint.http_url
            endpoint.save()

        vuln, _ = save_vulnerability(
            target_domain=self.domain,
            http_url=http_url,
            scan_history=self.scan,
            subscan=self.subscan,
            **vuln_data
        )

        if not vuln:
            continue

    # after vulnerability scan is done, we need to run gpt if
    # should_fetch_gpt_report and openapi key exists

    if should_fetch_gpt_report and OpenAiAPIKey.objects.all().first():
        logger.info('Getting CRLFuzz Vulnerability GPT Report')
        vulns = Vulnerability.objects.filter(
            scan_history__id=self.scan_id
        ).filter(
            source=CRLFUZZ
        ).exclude(
            severity=0
        )

        _vulns = []
        for vuln in vulns:
            _vulns.append((vuln.name, vuln.http_url))

        with concurrent.futures.ThreadPoolExecutor(max_workers=DEFAULT_THREADS) as executor:
            future_to_gpt = {executor.submit(get_vulnerability_gpt_report, vuln): vuln for vuln in _vulns}

            # Wait for all tasks to complete
            for future in concurrent.futures.as_completed(future_to_gpt):
                gpt = future_to_gpt[future]
                try:
                    future.result()
                except Exception as e:
                    logger.error(f"Exception for Vulnerability {vuln}: {e}")

    return []

@app.task(name='s3scanner', queue='s3scanner_queue', base=RengineTask, bind=True)
def s3scanner(self, ctx=None, description=None):
    """Bucket Scanner

    Args:
        ctx (dict): Context
        description (str, optional): Task description shown in UI.
    """
    if ctx is None:
        ctx = {}
    input_path = str(Path(self.results_dir) / f'{self.scan_id}_s3_bucket_discovery.txt')

    subdomains = Subdomain.objects.filter(scan_history=self.scan)
    if not subdomains:
        logger.error('No subdomains found for S3Scanner. Skipping.')
        return

    with open(input_path, 'w') as f:
        for subdomain in subdomains:
            f.write(subdomain.name + '\n')

    vuln_config = self.yaml_configuration.get(VULNERABILITY_SCAN) or {}
    s3_config = vuln_config.get(S3SCANNER) or {}
    threads = s3_config.get(THREADS) or self.yaml_configuration.get(THREADS, DEFAULT_THREADS)
    providers = s3_config.get(PROVIDERS, S3SCANNER_DEFAULT_PROVIDERS)
    scan_history = ScanHistory.objects.filter(pk=self.scan_id).first()
    for provider in providers:
        cmd = f's3scanner -bucket-file {input_path} -enumerate -provider {provider} -threads {threads} -json'
        for line in stream_command(
                cmd,
                history_file=self.history_file,
                scan_id=self.scan_id,
                activity_id=self.activity_id):

            if not isinstance(line, dict):
                continue

            if line.get('bucket', {}).get('exists', 0) == 1:
                result = parse_s3scanner_result(line)
                s3bucket, created = S3Bucket.objects.get_or_create(**result)
                scan_history.buckets.add(s3bucket)
                logger.info(f"s3 bucket added {result['provider']}-{result['name']}-{result['region']}")
